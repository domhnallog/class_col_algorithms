{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faulty Takata Airbags using Logistic Regression\n",
    "\n",
    "This story, done by The New York Times, investigates the content in complaints made to National Highway Traffic Safety Administration (NHTSA) by customers who had bad experiences with Takata airbags in their cars. Eventually, car companies had to recall airbags made by the airbag supplier that promised a cheaper alternative. \n",
    "\n",
    "- https://www.nytimes.com/2014/09/12/business/air-bag-flaw-long-known-led-to-recalls.html\n",
    "- https://www.nytimes.com/2014/11/07/business/airbag-maker-takata-is-said-to-have-conducted-secret-tests.html\n",
    "- https://www.nytimes.com/interactive/2015/06/22/business/international/takata-airbag-recall-list.html\n",
    "- https://www.nytimes.com/2016/08/27/business/takata-airbag-recall-crisis.html\n",
    "\n",
    "You can also see a presentation by Daeil Kim who did the machine learning side of the story, i.e. used logistic regression to classify whether a comment was relevant to the story or not. \n",
    "\n",
    "https://www.slideshare.net/mortardata/daeil-kim-at-the-nyc-data-science-meetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data and do basic exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CMPLID', 'ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', 'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', 'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', 'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', 'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE', 'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', 'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', 'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', 'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', 'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "df = pd.read_csv(\"data/takata/FLAT_CMPL.txt\",\n",
    "                 sep='\\t',\n",
    "                 dtype='str',\n",
    "                 header=None,\n",
    "                 error_bad_lines=False,\n",
    "                 encoding='latin-1',\n",
    "                 names=column_names)\n",
    "\n",
    "# We're only interested in pre-2015\n",
    "df = df[df.DATEA < '2015']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data do we have in this data set? How many columns do we have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 49 columns in this dataset, and many of them are not relevant to our analysis. If we keep them as \"features\", like we saw earlier with the Titanic dataset, we might get really skewed results. So, let's just only keep what we are interested in, and then create our features based on that. \n",
    "\n",
    "In this context, because we're working with unstructured data, we'll have to create our own columns. What do I mean by that? Well, it's hard for machines to infer _much_ from volumes of comments, so it means we have to make the machine's job easier. \n",
    "\n",
    "How can we do that?\n",
    "- Only care about comments that talk about airbags. We don't really care about any of the other comments for the purpose of this story, but we also don't want to overfit the model. \n",
    "- Figure out what words or combination of words would indicate an airbag issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of rows\n",
    "\n",
    "\n",
    "\n",
    "# Take a sample of 300 items involving airbags. \n",
    "# This is a fun bit to pay attention to: this is unstructured data, so people might type \"air bag\" or \"airbag\". \n",
    "# Let's factor that in. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine them so we have all sorts for the machine to learn about\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We don't know whether they're suspicious or not\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We only want a few columns\n",
    "\n",
    "\n",
    "\n",
    "# Save them\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once we've pulled out a sample of data, with and without airbags in the comments, we need to find the possible combination of words that indicate an airbag's not behaving as it should, i.e. if you see \"shrapnel\" or \"explode\" in the comment with airbags, you can probably infer that it's down to the faulty airbag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on the same of data can you see what terms of phrases are common when complaining about airbags? \n",
    "## Let's build some features out of those common phrases. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK, so we are lucky in that someone's actually labelled some of our sample to highlight whether the comment \n",
    "## indicates a faulty/suspicious airbag or not. Let's read that in, clean out the NAs, and see what we have. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, we create our training features matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what the breakdown is of suspicious vs. not in this dataset. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn, and follow the steps from Titanic. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, find the most important \"features\", and their corresponding coefficients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And see what our classifier scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's look at another way to do training vs. test data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As before, build out the confusion matrix. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, I want to start here next class, so I'll leave this here — it should be familiar to you for linear regression, \n",
    "# but we need to talk about interpreting coefficients for logistic regression, and what Odds Ratio—the very premise of \n",
    "# logistic regression—is. \n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = features.drop(columns='is_suspicious')\n",
    "X = sm.add_constant(X)\n",
    "y = features.is_suspicious\n",
    "\n",
    "model = sm.Logit(y, X)\n",
    "results = model.fit(method='lbfgs')\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
