{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lede Algorithms, Lecture 4. \n",
    "#### July 19, 2019. Friday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression\n",
    "\n",
    "We're going to be using logistic regression on a very very old albeit familiar dataset just to try and understand the various concepts around how we approach logistic regression problems. Logistic regression, which you may remember, is a classification algorithm. For the purpose of this class, we're restricting the scope to dichotomous or binary dependent variables, i.e. the categories your algorithm will predict will be one of two choices. The dataset: Titanic. The output: Survived? Yes or no. \n",
    "\n",
    "You might wonder why we are bothering to use this dataset again. Is it particularly interesting? And, what are the odds—no puns intended—of us ever using a dataset like this for journalistic reasons? Let's go through the exercise and then discuss the ethics of doing something like this on more current/live data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all the packages/modules that we need. \n",
    "# If you get a `ModuleNotFoundError`, run a `pip install` for the module \n",
    "# that failed to import.\n",
    "# You'll need to have the following Python modules installed:\n",
    "# - matplotlib\n",
    "# - numpy\n",
    "# - pandas\n",
    "# - seaborn\n",
    "# - sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data in. \n",
    "You should notice that we now have two different files: `train.csv` and `test.csv`. Wait, what, why? (This data has been downloaded from Kaggle: https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "`train.csv` is our training dataset, i.e. the dataset our algorithm will learn from. It has a bunch of features and the output label (i.e. survived: yes/no) for a subset of the passengers. The remaining passengers are in the `test.csv` file, i.e. they make up our test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12) (418, 12)\n"
     ]
    }
   ],
   "source": [
    "# Yes, we are doing this here, but remember, relative paths are evil. \n",
    "\n",
    "train_df = pd.read_csv('sources/titanic_train.csv')\n",
    "test_df = pd.read_csv('sources/titanic_test.csv')\n",
    "\n",
    "# In theory, both dataframes should have the same number of columns, but \n",
    "# the training dataset should have more rows. Let's do a quick sanity check\n",
    "# to see the kind of data we are dealing with. \n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and clean the data\n",
    "It's important to get a feel for the data, because unless you're not intimate with it, the chances of making a careless mistake are high. In this case, most of the column names are self-explanatory, but there are a couple that aren't. \n",
    "- `SibSp` defines family relationships for siblings and spouses\n",
    "- `Parch` defines family relationships for parents and children; when 0, it means the child was travelling with a nanny. \n",
    "\n",
    "Once we understand what each of the columns mean, let's dive into the data. Typically, folks are \n",
    "inclined to do a `df.head()`, but the problem with that is it often masks inconsistencies, so randomly sampling data can be a better approach. It's not 100% foolproof, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Boulos, Mrs. Joseph (Sultana)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2678</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Madsen, Mr. Fridtjof Arne</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C 17369</td>\n",
       "      <td>7.1417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Meyer, Mr. August</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248723</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Corn, Mr. Harry</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392090</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Francatelli, Miss. Laura Mabel</td>\n",
       "      <td>female</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17485</td>\n",
       "      <td>56.9292</td>\n",
       "      <td>E36</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>390</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Lehmann, Miss. Bertha</td>\n",
       "      <td>female</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SC 1748</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>682</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hassab, Mr. Hammad</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17572</td>\n",
       "      <td>76.7292</td>\n",
       "      <td>D49</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Windelov, Mr. Einar</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 3101317</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>358</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Funk, Miss. Annie Clemmer</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237671</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Danoff, Mr. Yoto</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349219</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>441</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hart, Mrs. Benjamin (Esther Ada Bloomfield)</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F.C.C. 13529</td>\n",
       "      <td>26.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>457</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Millet, Mr. Francis Davis</td>\n",
       "      <td>male</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13509</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>E38</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>711</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mayne, Mlle. Berthe Antonine (\"Mrs de Villiers\")</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17482</td>\n",
       "      <td>49.5042</td>\n",
       "      <td>C90</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Mr. Andrew G</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Smiljanic, Mr. Mile</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315037</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "140          141         0       3   \n",
       "127          128         1       3   \n",
       "808          809         0       2   \n",
       "157          158         0       3   \n",
       "309          310         1       1   \n",
       "389          390         1       2   \n",
       "681          682         1       1   \n",
       "491          492         0       3   \n",
       "357          358         0       2   \n",
       "321          322         0       3   \n",
       "440          441         1       2   \n",
       "456          457         0       1   \n",
       "710          711         1       1   \n",
       "783          784         0       3   \n",
       "158          159         0       3   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "140                     Boulos, Mrs. Joseph (Sultana)  female   NaN      0   \n",
       "127                         Madsen, Mr. Fridtjof Arne    male  24.0      0   \n",
       "808                                 Meyer, Mr. August    male  39.0      0   \n",
       "157                                   Corn, Mr. Harry    male  30.0      0   \n",
       "309                    Francatelli, Miss. Laura Mabel  female  30.0      0   \n",
       "389                             Lehmann, Miss. Bertha  female  17.0      0   \n",
       "681                                Hassab, Mr. Hammad    male  27.0      0   \n",
       "491                               Windelov, Mr. Einar    male  21.0      0   \n",
       "357                         Funk, Miss. Annie Clemmer  female  38.0      0   \n",
       "321                                  Danoff, Mr. Yoto    male  27.0      0   \n",
       "440       Hart, Mrs. Benjamin (Esther Ada Bloomfield)  female  45.0      1   \n",
       "456                         Millet, Mr. Francis Davis    male  65.0      0   \n",
       "710  Mayne, Mlle. Berthe Antonine (\"Mrs de Villiers\")  female  24.0      0   \n",
       "783                            Johnston, Mr. Andrew G    male   NaN      1   \n",
       "158                               Smiljanic, Mr. Mile    male   NaN      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "140      2              2678  15.2458   NaN        C  \n",
       "127      0           C 17369   7.1417   NaN        S  \n",
       "808      0            248723  13.0000   NaN        S  \n",
       "157      0   SOTON/OQ 392090   8.0500   NaN        S  \n",
       "309      0          PC 17485  56.9292   E36        C  \n",
       "389      0           SC 1748  12.0000   NaN        C  \n",
       "681      0          PC 17572  76.7292   D49        C  \n",
       "491      0  SOTON/OQ 3101317   7.2500   NaN        S  \n",
       "357      0            237671  13.0000   NaN        S  \n",
       "321      0            349219   7.8958   NaN        S  \n",
       "440      1      F.C.C. 13529  26.2500   NaN        S  \n",
       "456      0             13509  26.5500   E38        S  \n",
       "710      0          PC 17482  49.5042   C90        C  \n",
       "783      2        W./C. 6607  23.4500   NaN        S  \n",
       "158      0            315037   8.6625   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's see a quick sample of our data to see what we've got. \n",
    "train_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've had a quick glance, let's find out: \n",
    "- if any data is missing\n",
    "- if any data needs to be normalised \n",
    "- if any data needs to be removed altogether\n",
    "- ...and, let's make some decisions: do we really need all these columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we find attempt identifying missing data points. \n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, so a few things: \n",
    "- Only three columns (henceforth called \"features\") are missing data. What are they, and what % of data is missing?\n",
    "\n",
    "\n",
    "Note: anytime you think, \"we can fudge this,\" **PAUSE**. \n",
    "What are the consequences of fudging the data? What would happen if you got it wrong? How much would it undermine everything else you've done? \n",
    "\n",
    "\n",
    "But, also, a passenger's PassengerID, name, or family relationships should hold no bearing on whether they survived the disaster or not. So, maybe we can drop those columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the columns we don't need so that we can focus on what we do need. \n",
    "# remember: whatever action we take on the training data, we need to do exactly the same for the test data. \n",
    "\n",
    "train_df = train_df.drop(['PassengerId','Name','Parch','SibSp'], axis=1)\n",
    "test_df = test_df.drop(['PassengerId','Name','Parch','SibSp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the two missing \"Embarked\" values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NA indices for Embarked are at: Int64Index([61, 829], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  Ticket  Fare Cabin Embarked\n",
       "61          1       1  female  38.0  113572  80.0   B28      NaN\n",
       "829         1       1  female  62.0  113572  80.0   B28      NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first look at the data, to see if we can see anything obvious. \n",
    "# For example, can the family data help us extrapolate?\n",
    "\n",
    "print(f\"The NA indices for Embarked are at: {train_df.index[train_df['Embarked'].isna()]}\")\n",
    "train_df.iloc[[61, 829]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's look at the missing data points, and see what we can do. \n",
    "# Starting with Embarked...\n",
    "### What's the breakdown of values in Embarked?\n",
    "\n",
    "train_df.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we breakdown \"Embarked\" by the counts, we see 'S' is the most common, so we blindly adopt that. \n",
    "# We can blindly do this for two reasons:\n",
    "# (i) the number of missing data points is tiiiiiiny \n",
    "# (ii) does the port where someone embarked _realistically_ impact their survival? \n",
    "\n",
    "train_df.Embarked = train_df.Embarked.fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the missing data in Cabin?\n",
    "\n",
    "If you look through one of the references, you'll see that they use what they have and ignore what they don't. But, that doesn't seem entirely right, because it might skew our results one way or another. So, let's drop it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Cabin from our training and test data sets\n",
    "\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the missing data in Age?\n",
    "\n",
    "Age is obviously a far more crucial datapoint. The approaches adopted by the references vary. \n",
    "- Using the median age based on the data where age != NA\n",
    "- Computing a random age based on the mean age and standard deviation\n",
    "\n",
    "For the sake of simplicity, let's go down the route of option 1. Again, remember, we need to mimic this across the training & test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we fill the median age on the test dataframe, we stick to the median\n",
    "# age we computed from the training data. This is because we shouldn't be\n",
    "# doing _anything_ based on the test dataset. We might end up skewing the \n",
    "# results or overfitting, and we need to be careful to avoid that. \n",
    "\n",
    "train_df[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\n",
    "test_df[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\n",
    "train_df[\"Fare\"].fillna(0, inplace=True)\n",
    "test_df[\"Fare\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "For logistic regression to work, the data needs to be \"normalised\". By that, we mean:\n",
    "- we can't have string values, i.e. gender which is currently male/female should be numeric. \n",
    "- individual ages might lead to _overfitting_, so we create age brackets. \n",
    "- ...and we might want to see what we can do with the tickets, because they might be unique ticket IDs, which means it wouldn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601            7\n",
       "CA. 2343        7\n",
       "347082          7\n",
       "3101295         6\n",
       "347088          6\n",
       "CA 2144         6\n",
       "S.O.C. 14879    5\n",
       "382652          5\n",
       "17421           4\n",
       "349909          4\n",
       "113760          4\n",
       "4133            4\n",
       "LINE            4\n",
       "113781          4\n",
       "347077          4\n",
       "PC 17757        4\n",
       "19950           4\n",
       "W./C. 6608      4\n",
       "2666            4\n",
       "110413          3\n",
       "C.A. 31921      3\n",
       "C.A. 34651      3\n",
       "345773          3\n",
       "248727          3\n",
       "PC 17760        3\n",
       "35273           3\n",
       "230080          3\n",
       "PC 17572        3\n",
       "347742          3\n",
       "29106           3\n",
       "               ..\n",
       "2926            1\n",
       "342826          1\n",
       "349205          1\n",
       "PC 17482        1\n",
       "365222          1\n",
       "14311           1\n",
       "244310          1\n",
       "236171          1\n",
       "A/5. 2151       1\n",
       "239855          1\n",
       "17764           1\n",
       "364846          1\n",
       "234604          1\n",
       "350025          1\n",
       "334912          1\n",
       "315086          1\n",
       "237668          1\n",
       "2697            1\n",
       "17463           1\n",
       "112058          1\n",
       "2693            1\n",
       "315037          1\n",
       "349910          1\n",
       "392092          1\n",
       "SC 1748         1\n",
       "2624            1\n",
       "349244          1\n",
       "349201          1\n",
       "19988           1\n",
       "7546            1\n",
       "Name: Ticket, Length: 681, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's start with tickets, and see how many unique values we have\n",
    "\n",
    "train_df.Ticket.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, you can immediately tell this isn't going to fly: 691 unique items here \n",
    "# will lead to overfitting. So, let's drop this. \n",
    "\n",
    "train_df = train_df.drop(['Ticket'], axis=1)\n",
    "test_df = test_df.drop(['Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      577\n",
       "female    314\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next up, genders. Let's check what our gender counts are, and then decide \n",
    "# on unique values for each gender. \n",
    "\n",
    "train_df.Sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, so that's pretty straightforward: add a new column to the dataframe: 'Male'. \n",
    "# If male, this column should have a value of 1. Else 0. \n",
    "\n",
    "train_df[\"Male\"] = train_df.Sex.apply(lambda x: 1 if x=='male' else 0)\n",
    "test_df[\"Male\"] = test_df.Sex.apply(lambda x: 1 if x=='male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, now, let's do age brackets. How should we do this? 'Adult'/'Child' to\n",
    "# keep it simple? Or, proper brackets? \n",
    "# As the folks at TowardsDataScience did, let's do age brackets. (And, yes, \n",
    "# this cell is blindly copying & pasting their code.)\n",
    "\n",
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    dataset['Age_Cat'] = dataset['Age'].astype(int)\n",
    "    dataset.loc[ dataset['Age_Cat'] <= 11, 'Age_Cat'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age_Cat'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age_Cat'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age_Cat'] = 3\n",
    "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age_Cat'] = 4\n",
    "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age_Cat'] = 5\n",
    "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age_Cat'] = 6\n",
    "    dataset.loc[ dataset['Age'] > 66, 'Age_Cat'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do the same for fares. \n",
    "for dataset in data:\n",
    "    dataset['Fare_Cat'] = dataset['Fare'].astype(int)\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare_Cat'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare_Cat'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare_Cat']   = 2\n",
    "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare_Cat']   = 3\n",
    "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare_Cat']   = 4\n",
    "    dataset.loc[ dataset['Fare'] > 250, 'Fare_Cat'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, because Embarked is a single letter, let's just map it to int, too. \n",
    "# This is cheating somewhat. Instead of going through the values, and assigning\n",
    "# [1, 2, 3], what we are doing is simply mapping the letter to its ordinal value.\n",
    "\n",
    "train_df[\"Embarked_Cat\"] = train_df.Embarked.apply(lambda x: ord(x))\n",
    "test_df[\"Embarked_Cat\"] = test_df.Embarked.apply(lambda x: ord(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...and now we can drop sex and age from the original data frames as we have\n",
    "#the normalised columns in there. \n",
    "\n",
    "train_df = train_df.drop(['Fare','Sex','Age','Embarked'], axis=1)\n",
    "test_df = test_df.drop(['Fare','Sex','Age','Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "OK, we finally have data that we can run logistic regression on. So, here's what we have to do: \n",
    "1. Split our data frames into X_train, y_train, X_test, and y_test. This, in English, means, training data, labels for the training data, test data, and labels for test data. \"X\" is shorthand for training/input data, and \"y\" is shorthand for labels. In our case, our X will be all columns in our dataframe except Survived, and our y will be the Survived columns. \n",
    "2. Once we do that, we simply run the data as is through the sklearn LogisticRegression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our arrays, which are parameters to the LogisticRegression classifier. \n",
    "\n",
    "X_train = train_df.drop('Survived', axis=1)\n",
    "Y_train = train_df['Survived']\n",
    "X_test = test_df.drop('Survived', axis=1)\n",
    "Y_test = test_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8588516746411483\n",
      "Precision: 0.8175675675675675\n",
      "Recall: 0.7908496732026143\n",
      "Confusion Matrix: \n",
      " [[238  27]\n",
      " [ 32 121]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train , Y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "print(f\"Accuracy: {metrics.accuracy_score(Y_test, predictions)}\")\n",
    "print(f\"Precision: {metrics.precision_score(Y_test, predictions)}\")\n",
    "print(f\"Recall: {metrics.recall_score(Y_test, predictions)}\")\n",
    "print(f\"Confusion Matrix: \\n {metrics.confusion_matrix(Y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n",
    "- https://www.kaggle.com/mnassrib/titanic-logistic-regression-with-python/data#1.-Import-Data-&-Python-Packages\n",
    "- https://www.kaggle.com/kernels/svzip/447794\n",
    "- https://github.com/jstray/lede-algorithms/tree/master/week-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
