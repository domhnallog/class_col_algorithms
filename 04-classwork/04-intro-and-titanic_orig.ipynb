{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lede Algorithms, Lecture 4. \n",
    "#### July 19, 2019. Friday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression\n",
    "\n",
    "We're going to be using logistic regression on a very very old albeit familiar dataset just to try and understand the various concepts around how we approach logistic regression problems. Logistic regression, which you may remember, is a classification algorithm. For the purpose of this class, we're restricting the scope to dichotomous or binary dependent variables, i.e. the categories your algorithm will predict will be one of two choices. The dataset: Titanic. The output: Survived? Yes or no. \n",
    "\n",
    "You might wonder why we are bothering to use this dataset again. Is it particularly interesting? And, what are the odds—no puns intended—of us ever using a dataset like this for journalistic reasons? Let's go through the exercise and then discuss the ethics of doing something like this on more current/live data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import all the packages/modules that we need. \n",
    "# If you get a `ModuleNotFoundError`, run a `pip install` for the module \n",
    "# that failed to import.\n",
    "# You'll need to have the following Python modules installed:\n",
    "# - matplotlib\n",
    "# - numpy\n",
    "# - pandas\n",
    "# - seaborn\n",
    "# - sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data in. \n",
    "You should notice that we now have two different files: `train.csv` and `test.csv`. Wait, what, why? (This data has been downloaded from Kaggle: https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "`train.csv` is our training dataset, i.e. the dataset our algorithm will learn from. It has a bunch of features and the output label (i.e. survived: yes/no) for a subset of the passengers. The remaining passengers are in the `test.csv` file, i.e. they make up our test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, we are doing this here, but remember, relative paths are evil. \n",
    "\n",
    "\n",
    "\n",
    "# In theory, both dataframes should have the same number of columns, but \n",
    "# the training dataset should have more rows. Let's do a quick sanity check\n",
    "# to see the kind of data we are dealing with. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and clean the data\n",
    "It's important to get a feel for the data, because unless you're not intimate with it, the chances of making a careless mistake are high. In this case, most of the column names are self-explanatory, but there are a couple that aren't. \n",
    "- `SibSp` defines family relationships for siblings and spouses\n",
    "- `Parch` defines family relationships for parents and children; when 0, it means the child was travelling with a nanny. \n",
    "\n",
    "Once we understand what each of the columns mean, let's dive into the data. Typically, folks are \n",
    "inclined to do a `df.head()`, but the problem with that is it often masks inconsistencies, so randomly sampling data can be a better approach. It's not 100% foolproof, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see a quick sample of our data to see what we've got. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've had a quick glance, let's find out: \n",
    "- if any data is missing\n",
    "- if any data needs to be normalised \n",
    "- if any data needs to be removed altogether\n",
    "- ...and, let's make some decisions: do we really need all these columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we find attempt identifying missing data points. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, so a few things: \n",
    "- Only three columns (henceforth called \"features\") are missing data. What are they, and what % of data is missing?\n",
    "\n",
    "\n",
    "Note: anytime you think, \"we can fudge this,\" **PAUSE**. \n",
    "What are the consequences of fudging the data? What would happen if you got it wrong? How much would it undermine everything else you've done? \n",
    "\n",
    "\n",
    "But, also, a passenger's PassengerID, name, or family relationships should hold no bearing on whether they survived the disaster or not. So, maybe we can drop those columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the columns we don't need so that we can focus on what we do need. \n",
    "# remember: whatever action we take on the training data, we need to do exactly the same for the test data. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the two missing \"Embarked\" values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first look at the data, to see if we can see anything obvious. \n",
    "# For example, can the family data help us extrapolate?\n",
    "print(f\"The NA indices for Embarked are at: {train_df.index[train_df['Embarked'].isna()]}\")\n",
    "train_df.iloc[[61, 829]]\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's look at the missing data points, and see what we can do. \n",
    "# Starting with Embarked...\n",
    "### What's the breakdown of values in Embarked?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# When we breakdown \"Embarked\" by the counts, we see 'S' is the most common, so we blindly adopt that. \n",
    "# We can blindly do this for two reasons:\n",
    "# (i) the number of missing data points is tiiiiiiny \n",
    "# (ii) does the port where someone embarked _realistically_ impact their survival? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the missing data in Cabin?\n",
    "\n",
    "If you look through one of the references, you'll see that they use what they have and ignore what they don't. But, that doesn't seem entirely right, because it might skew our results one way or another. So, let's drop it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Cabin from our training and test data sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we solve the missing data in Age?\n",
    "\n",
    "Age is obviously a far more crucial datapoint. The approaches adopted by the references vary. \n",
    "- Using the median age based on the data where age != NA\n",
    "- Computing a random age based on the mean age and standard deviation\n",
    "\n",
    "For the sake of simplicity, let's go down the route of option 1. Again, remember, we need to mimic this across the training & test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we fill the median age on the test dataframe, we stick to the median\n",
    "# age we computed from the training data. This is because we shouldn't be\n",
    "# doing _anything_ based on the test dataset. We might end up skewing the \n",
    "# results or overfitting, and we need to be careful to avoid that. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "For logistic regression to work, the data needs to be \"normalised\". By that, we mean:\n",
    "- we can't have string values, i.e. gender which is currently male/female should be numeric. \n",
    "- individual ages might lead to _overfitting_, so we create age brackets. \n",
    "- ...and we might want to see what we can do with the tickets, because they might be unique ticket IDs, which means it wouldn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with tickets, and see how many unique values we have\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, you can immediately tell this isn't going to fly: 691 unique items here \n",
    "# will lead to overfitting. So, let's drop this. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up, genders. Let's check what our gender counts are, and then decide \n",
    "# on unique values for each gender. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, so that's pretty straightforward: add a new column to the dataframe: 'Male'. \n",
    "# If male, this column should have a value of 1. Else 0. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, now, let's do age brackets. How should we do this? 'Adult'/'Child' to\n",
    "# keep it simple? Or, proper brackets? \n",
    "# As the folks at TowardsDataScience did, let's do age brackets. (And, yes, \n",
    "# this cell is blindly copying & pasting their code.)\n",
    "\n",
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    dataset['Age Category'] = dataset['Age'].astype(int)\n",
    "    dataset.loc[ dataset['Age Category'] <= 11, 'Age Category'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age Category'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age Category'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age Category'] = 3\n",
    "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age Category'] = 4\n",
    "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age Category'] = 5\n",
    "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age Category'] = 6\n",
    "    dataset.loc[ dataset['Age'] > 66, 'Age Category'] = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do the same for fares. \n",
    "for dataset in data:\n",
    "    dataset['Fare Category'] = dataset['Fare'].astype(int)\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare Category'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare Category'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare Category']   = 2\n",
    "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare Category']   = 3\n",
    "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare Category']   = 4\n",
    "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, because Embarked is a single letter, let's just map it to int, too. \n",
    "# This is cheating somewhat. Instead of going through the values, and assigning\n",
    "# [1, 2, 3], what we are doing is simply mapping the letter to its ordinal value.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...and now we can drop sex and age from the original data frames as we have\n",
    "#the normalised columns in there. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "OK, we finally have data that we can run logistic regression on. So, here's what we have to do: \n",
    "1. Split our data frames into X_train, y_train, X_test, and y_test. This, in English, means, training data, labels for the training data, test data, and labels for test data. \"X\" is shorthand for training/input data, and \"y\" is shorthand for labels. In our case, our X will be all columns in our dataframe except Survived, and our y will be the Survived columns. \n",
    "2. Once we do that, we simply run the data as is through the sklearn LogisticRegression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our arrays, which are parameters to the LogisticRegression classifier. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n",
    "- https://www.kaggle.com/mnassrib/titanic-logistic-regression-with-python/data#1.-Import-Data-&-Python-Packages\n",
    "- https://www.kaggle.com/kernels/svzip/447794\n",
    "- https://github.com/jstray/lede-algorithms/tree/master/week-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
