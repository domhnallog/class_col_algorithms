{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leftovers\n",
    "\n",
    "So there were two unanswered questions at the end of last class. Let's get into both of them. \n",
    "- Why did 'a' not get included in the `CountVectorizer`? And, if we wanted to, how do we include 'a'? \n",
    "- Why do the TF-IDF weights look...not sensible? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizers, again\n",
    "\n",
    "With most of our `sklearn` classes and modules, there are some default values that will kick in when we don't specify anything. We saw a couple of examples of this with n-grams last week. If you don't specify `ngram_range` to the `CountVectorizer`, it will assume you only want to do `unigrams`. Similarly, if you don't specify a `stop_words` list, it will assume you don't want to factor stop words into your analysis. \n",
    "\n",
    "Other parameters that the `CountVectorizer` cares about include:\n",
    "- `lowercase` (by default, True)\n",
    "- `min_df` (by default, 1.0)\n",
    "- `max_df` (by default, 1.0)\n",
    "- `max_features` (by default, None, i.e. _everything_ is a feature). \n",
    "\n",
    "So, where does 'a' come in? By another parameter called `token_pattern`. The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (always always read the documentation!) suggests (emphasis mine): \n",
    "> Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. **The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)**. \n",
    "\n",
    "This means that single-character words ('A', 'I') will get dropped by the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_titles = [\n",
    "    \"Candle in the wind\", \n",
    "    \"A pillow of winds\", \n",
    "    \"Wind of change\",\n",
    "    \"The wind cries Mary\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Right, now, how can we get our CountVectorizer to include it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the `token_pattern` is the parameter that's eliminating single-character words. So, let's have a look at the what the _actual_ value for `token_pattern` is. We can do this by simply running `cv.token_pattern`, which gives us the below result:\n",
    "\n",
    "> `'(?u)\\\\b\\\\w\\\\w+\\\\b'`\n",
    "\n",
    "![alt text](files/regex.png)\n",
    "\n",
    "Source: https://www.xkcd.com/208/\n",
    "    \n",
    "    \n",
    "OK, so we want to make a minor change to the above regex to include single-character words. How do we do it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF, again\n",
    "Right, so, we know TF-IDF means \"term frequency inverse document frequency.\" The calculation is:\n",
    "\n",
    "> `tf * idf`\n",
    "\n",
    " But, how do you calculate `tf` and `idf` respectively? \n",
    " \n",
    "> `tf(t)` = `(# times term appears in document) ÷ (total # of words in document)`\n",
    "\n",
    "For example, if you had a document with a single line: \"Data, data, data. It's all about the data.\" Here, the `tf(t='data')` would be count('data') / count(# words), i.e. 4/8 or 1/2. \n",
    "\n",
    "Next up, we have \"inverse document frequency\" or \"idf\". The calculation for `idf` is:\n",
    "> ` log [ n / df(d, t) + 1 ] + 1 `\n",
    "\n",
    "...where: \n",
    "- `n` is the # of documents\n",
    "- `df(d, t)` is the number of documents containing the term\n",
    "\n",
    "Let's go back to our song titles toy example below to see how it works with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for the purposes of this walkthrough, we aren't using stopwords or a stemmer. We'll get to that in a few cells. Now, let's take the example of 'Candle in the Wind'. The term frequency for each of the terms are 1/4, i.e.:\n",
    "- `tf('candle') = tf('in') = tf('the') = tf('wind')` = 1/4 (because each term appears only once)\n",
    "\n",
    "Let's now do the whole 'tfidf' for 'candle' and 'wind':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm if our IDF calculations are accurate by checking what `TfidfVectorizer` calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so, now we can multiply our term frequency with our inverse document frequency. There's another _implicit_ variable here called the loss, which we won't get into, but that will basically mean our final calculations will be _slightly_ different from the sklearn class.\n",
    "\n",
    "We can now replicate this with the stemmed TfidfVectorizer that uses stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(word) for word in analyzer(doc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the IDF values calculated are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
